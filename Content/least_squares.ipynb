{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares - theoretical aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents some theoretical developments for deducing the Full-Rank Least Squares (Golub and Van Loan, 2013, p. 260) or simply Least Squares Problem (Menke and Menke, 2016, p. 80; Menke, 2018, p. 44; Aster et al., 2019, p. 26). Here, we explore the normal equations method (Golub and Van Loan, 2013, p. 262). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Golub, G. H. and Van Loan, C. F. Matrix computations, 4th edition, Johns Hopkins University Press, 2013, ISBN 978-1-4214-0859-0\n",
    "\n",
    "* Menke, W. and Menke, J. Environmental Data Analysis with MATLAB, 2nd edition, Academic Press, 2016, ISBN 978-0-12-804488-9\n",
    "\n",
    "* Menke, W. Geophysical Data Analysis: Discrete inverse theory, 4th edition, Academic Press, 2018, ISBN 978-0-12-813555-6\n",
    "\n",
    "* Aster, R. C., Borchers, B., and Thurber, C. H. Parameter Estimation and Inverse Problems, 3rd edition Elsevier Academic Press, 2019, ISBN: 978-0-12-804651-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a set of measurements $d_{i}$, $i = 0, \\dots, N-1$, of a given physical quantity. **The measurements** $d_{i}$ **are usually called observed data**. Let's also consider that each observed data $d_{i}$ can be properly approximated by a function $y_{i}$, $i = 0, \\dots, N-1$, given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq1'></a>\n",
    "$$\n",
    "y_{i} = a_{i0} \\, p_{0} + a_{i1} \\, p_{1} + \\cdots + a_{i(M-1)} \\, p_{(M-1)} \\: , \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $a_{ij}$ are known variables and $p_{j}$ are unknown variables, $i = 0, \\dots, N-1$, $j = 0, \\dots, M-1$, $N \\gt M$. **The** $y_{i}$ **are usually called predicted data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by considering the $N$ measurements, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq2'></a>\n",
    "$$\n",
    "\\begin{split}\n",
    "d_{0} &\\approx \\; &y_{0} &= \\: &a_{00} \\, p_{0} + &a_{01} \\, p_{1} + \\cdots + &a_{0(M-1)} \\, p_{M-1} \\\\\n",
    "d_{1} &\\approx &y_{1} &= &a_{10} \\, p_{0} + &a_{11} \\, p_{1} + \\cdots + &a_{1(M-1)} \\, p_{M-1} \\\\\n",
    "\\vdots & &\\vdots & & \\: \\vdots & \\: \\vdots & \\: \\vdots \\\\\n",
    "d_{N-1} &\\approx &y_{N-1} &= &a_{(N-1)0} \\, p_{0} + &a_{(N-1)1} \\, p_{1} + \\cdots + &a_{(N-1)(M-1)} \\, p_{M-1}\n",
    "\\end{split} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, equivalently,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq3'></a>\n",
    "$$\n",
    "\\mathbf{d} \\approx \\mathbf{y} = \\mathbf{A} \\, \\mathbf{p} \\: , \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq4'></a>\n",
    "$$\n",
    "\\mathbf{p} = \n",
    "\\left[ \\begin{array}{c}\n",
    "p_{0} \\\\\n",
    "p_{1} \\\\\n",
    "\\vdots \\\\\n",
    "p_{M-1}\n",
    "\\end{array} \\right] \\: , \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq5'></a>\n",
    "$$\n",
    "\\mathbf{y} = \n",
    "\\left[ \\begin{array}{c}\n",
    "y_{0} \\\\\n",
    "y_{1} \\\\\n",
    "\\vdots \\\\\n",
    "y_{N-1}\n",
    "\\end{array} \\right] \\: , \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq6'></a>\n",
    "$$\n",
    "\\mathbf{d} = \n",
    "\\left[ \\begin{array}{c}\n",
    "d_{0} \\\\\n",
    "d_{1} \\\\\n",
    "\\vdots \\\\\n",
    "d_{N-1}\n",
    "\\end{array} \\right] \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq7'></a>\n",
    "$$\n",
    "\\mathbf{A} = \n",
    "\\left[ \\begin{array}{cccc}\n",
    "a_{00} & a_{01} & \\cdots & a_{0(M-1)} \\\\\n",
    "a_{10} & a_{11} & \\cdots & a_{1(M-1)} \\\\\n",
    "\\vdots & \\vdots &        & \\vdots \\\\\n",
    "a_{(N-1)0} & a_{(N-1)1} & \\cdots & a_{(N-1)(M-1)}\n",
    "\\end{array} \\right] \\quad . \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors $\\mathbf{p}$, $\\mathbf{y}$, and $\\mathbf{d}$ are usually called **parameter vector**, **predicted data vector**, and **observed data vector**, respectively. Notice that determining the parameter vector $\\mathbf{p}$ from $\\mathbf{d}$ and $\\mathbf{A}$ requires to solve a non-square linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider the problem of determining $\\mathbf{p}$ from the observed data $\\mathbf{d}$ and the $N \\times M$ matrix $\\mathbf{A}$. Mathematically, this problem consists in determining a vector $\\mathbf{p} = \\tilde{\\mathbf{p}}$ producing a $\\mathbf{y}$ \"*as close as possible*\" to $\\mathbf{d}$. To solve this problem, we need to define what \"*as close as possible*\" means. The notion of \"*closeness*\" is intrinsically related to the notion of \"*distance*\" and, consequently, to the notion of <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)\">norm</a>.\n",
    "\n",
    "For example, let's consider a vector $\\mathbf{r} = \\mathbf{d} - \\mathbf{y}$, which is defined as the difference between the observed data $\\mathbf{d}$ and the predicted data $\\mathbf{y}$. The vector $\\mathbf{r}$ is usually called **residuals vector**. The length of the residuals vector can be determined by the following scalar function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq8'></a>\n",
    "$$\n",
    "\\begin{split}\n",
    "\\| \\mathbf{r} \\|_{2} &= \\sqrt{\\mathbf{r}^{\\top}\\mathbf{r}} \\\\\n",
    "&= \\sqrt{\\sum \\limits_{i = 0}^{N-1} r_{i}^{2}}\n",
    "\\end{split} \\: , \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $r_{i} = d_{i} - y_{i}$, $i = 0, \\dots, N-1$. This function is a norm that quantifies the \"*distance*\" between the vectors $\\mathbf{d}$ and $\\mathbf{y}$. It is called **<a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm\">Euclidean norm</a>** (or **Euclidean distance**). Notice that this function equals to zero if $\\mathbf{d} = \\mathbf{y}$ and it is greater than zero if $\\mathbf{d} \\ne \\mathbf{y}$.\n",
    "\n",
    "So, the problem of determining a vector $\\mathbf{p} = \\tilde{\\mathbf{p}}$ producing a $\\mathbf{y}$ \"*as close as possible*\" to $\\mathbf{d}$ can be thought of the problem of determining a parameter vector $\\mathbf{p} = \\tilde{\\mathbf{p}}$ producing the minimum Euclidean norm of the difference between the observed data $\\mathbf{d}$ and the predicted data $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical situations, instead of determining the vector $\\tilde{\\mathbf{p}}$ minimizing the Euclidean norm of $\\mathbf{r}$, we determine the vector $\\tilde{\\mathbf{p}}$ minimizing the **Squared Euclidean norm** (or **<a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Squared Euclidean distance</a>** or $L_{2}$ norm) of $\\mathbf{r}$, which is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq9'></a>\n",
    "$$\n",
    "\\begin{split}\n",
    "\\| \\mathbf{r} \\|_{2}^{2} &= \\mathbf{r}^{\\top}\\mathbf{r} \\\\\n",
    "&= \\sum \\limits_{i = 0}^{N-1} r_{i}^{2}\n",
    "\\end{split} \\: . \\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful! According to the mathematical definition of norm, the **Squared Euclidean norm** (or **<a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Squared Euclidean distance</a>**) is not a norm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the $L_{2}$ norm of $\\mathbf{r}$ is a scalar function depending on the unknows $\\mathbf{p}$ and can be written as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq10'></a>\n",
    "$$\n",
    "\\Phi(\\mathbf{p}) = \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right]^{\\top}\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] \\: . \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is usually called **misfit function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering that there is only one particular parameter vector $\\tilde{\\mathbf{p}}$ minimizing the misfit function $\\Phi(\\mathbf{p})$, we can state that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq11'></a>\n",
    "$$\n",
    "\\Phi(\\tilde{\\mathbf{p}} + \\Delta \\mathbf{p})\n",
    "\\begin{cases}\n",
    "\\gt \\Phi(\\tilde{\\mathbf{p}}) \\, , \\: \\text{if} \\:\\: \\| \\Delta \\mathbf{p}\\|_{2} \\ne 0 \\\\\n",
    "= \\Phi(\\tilde{\\mathbf{p}}) \\, , \\: \\text{if} \\:\\: \\| \\Delta \\mathbf{p}\\|_{2} = 0 \\\\\n",
    "\\end{cases} \\quad . \\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use the misfit function to estimate the particular parameter vector $\\tilde{\\mathbf{p}}$. To do it, we may simply compute the misfit function produced by a set of given parameter vectors and choose the particular parameter vector producing the minimum misfit function value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may estimate the parameter vector $\\tilde{\\mathbf{p}}$ by exploring the derivatives of the misfit function with respect to the components $p_{j}$, $j = 1, \\dots, M$, of the parameter vector. We know that, if the misfit function assumes a minimum value at $\\tilde{\\mathbf{p}}$, then its gradient evaluated at $\\tilde{\\mathbf{p}}$ equals to the null vector. It means that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq12'></a>\n",
    "$$\n",
    "\\nabla \\Phi(\\tilde{\\mathbf{p}}) = \n",
    "\\left[ \\begin{array}{c}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array} \\right] \\: , \\tag{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq13'></a>\n",
    "$$\n",
    "\\nabla \\Phi(\\mathbf{p}) = \n",
    "\\left[ \\begin{array}{c}\n",
    "\\dfrac{\\partial \\, \\Phi(\\mathbf{p})}{\\partial \\, p_{0}} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial \\, \\Phi(\\mathbf{p})}{\\partial \\, p_{M-1}} \n",
    "\\end{array} \\right] \\tag{13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the gradient of $\\Phi(\\mathbf{p})$ ([equation 10](#eq10)). Maybe, it is time to review some mathematical stuff to go forward with our study. Please, take a look at the notebooks `review_math_XXX`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $j$ th element of the gradient $\\nabla \\Phi(\\mathbf{p})$ ([equation 12](#eq12)) is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='eq14'></a>\n",
    "$$\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\, \\Phi(\\mathbf{p})}{\\partial \\, p_{j}} &= \n",
    "\\Big \\{ \\dfrac{\\partial}{\\partial \\, p_{j}} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] \\Big \\} \n",
    "^{\\top}\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] +\n",
    "\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right]^{\\top}\n",
    "\\Big \\{ \\dfrac{\\partial}{\\partial \\, p_{j}} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] \\Big \\} \\\\\n",
    "&= 2\\Big \\{ \\dfrac{\\partial}{\\partial \\, p_{j}} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] \\Big \\} \n",
    "^{\\top}\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] \\\\\n",
    "&= 2 \\Big \\{ -\\mathbf{u}_{j}^{\\top}\\mathbf{A}^{\\top} \\Big \\} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right]\n",
    "\\end{split} \\: , \\tag{14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{u}_{j}$ is a $M \\times 1$ vector whose $j$ th element is equal to $1$ and all the remaining elements are equal to $0$. By substituting this $\\frac{\\partial \\, \\Phi(\\mathbf{p})}{\\partial \\, p_{j}}$ ([equation 14](#eq14)) into the gradient equation ([equation 13](#eq13)), we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq15'></a>\n",
    "$$\n",
    "\\nabla \\Phi(\\mathbf{p}) = -2 \\mathbf{A}^{\\top} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{p} \\right] \\: . \\tag{15}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by evaluating the gradient $\\nabla \\Phi(\\mathbf{p})$ ([equation 13](#eq13)) at $\\mathbf{p} = \\tilde{\\mathbf{p}}$ ([equation 11](#eq11)), we obtain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq16'></a>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\Phi(\\tilde{\\mathbf{p}}) &= -2 \\mathbf{A}^{\\top} \\left[ \\mathbf{d} - \\mathbf{A}\\tilde{\\mathbf{p}} \\right] \\tag{16a} \\\\\n",
    "\\mathbf{0} &= -\\mathbf{A}^{\\top}\\mathbf{d} + \\mathbf{A}^{\\top}\\mathbf{A}\\tilde{\\mathbf{p}} \\tag{16b}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq17'></a>\n",
    "$$\n",
    "\\left( \\mathbf{A}^{\\top}\\mathbf{A} \\right) \\tilde{\\mathbf{p}} = \\mathbf{A}^{\\top}\\mathbf{d} \\: . \\tag{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation is commonly called **Least Squares Estimator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, to determine the vector $\\tilde{\\mathbf{p}}$, it is needed to solve a square linear system with a symmetric matrix $\\mathbf{A}^{\\top}\\mathbf{A}$ and an independent vector $\\mathbf{A}^{\\top}\\mathbf{d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the uncertainty of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider two $N \\times 1$ vectors $\\mathbf{t}$ and $\\mathbf{v}$, whose elements $t_{i}$ and $v_{i}$, $i = 0, \\dots, N-1$, respectively, are [random variables](https://en.wikipedia.org/wiki/Random_variable). Let's also consider that these vectors are related as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq18'></a>\n",
    "$$\n",
    "\\mathbf{t} = \\mathbf{H} \\mathbf{v} + \\mathbf{c} \\: , \\tag{18}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{H}$ and $\\mathbf{c}$ are, respectively, an $N \\times N$ matrix and an $N \\times 1$ vector formed by constant (non-random) elements. In this case, the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of $\\mathbf{t}$ (denoted by $\\mathbf{\\Sigma_{t}}$) can be computed by using the covariance matrix of $\\mathbf{v}$ (denoted by $\\mathbf{\\Sigma_{v}}$) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq19'></a>\n",
    "$$\n",
    "\\mathbf{\\Sigma_t} = \\mathbf{H} \\, \\mathbf{\\Sigma_{v}} \\mathbf{H}^{\\top} \\: . \\tag{19}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices $\\mathbf{\\Sigma_{t}}$ and $\\mathbf{\\Sigma_{v}}$ ([equation 19](#eq19)) are symmetric, with elements $ij$ representing the [covariance](https://en.wikipedia.org/wiki/Covariance) between the $i$-th and $j$-th elements of $\\mathbf{t}$ and $i$-th and $j$-th elements of $\\mathbf{v}$, respectively. The elements $ii$ on their main diagonals represent the [variances](https://en.wikipedia.org/wiki/Variance) of the elements of $\\mathbf{t}$ and $\\mathbf{v}$.\n",
    "\n",
    "If all elements $ij$ outside the main diagonal of $\\mathbf{\\Sigma_{v}}$ are zero, we say that the elements of $\\mathbf{v}$ are uncorrelated. If we additionally consider that all elements of $\\mathbf{v}$ have the same variance $\\sigma_{\\mathbf{v}}^{2}$, the covariance matrix $\\mathbf{\\Sigma_{v}}$ assumes the particular form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq20a'></a>\n",
    "$$\n",
    "\\mathbf{\\Sigma_v} = \\sigma_{\\mathbf{v}}^{2} \\mathbf{I} \\: , \\tag{20a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{I}$ represents the identity. By substituting [equation 20a](#eq20a) into [equation 19](#eq19), we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq20b'></a>\n",
    "$$\n",
    "\\mathbf{\\Sigma_t} = \\sigma_{\\mathbf{v}}^{2} \\, \\mathbf{H} \\, \\mathbf{H}^{\\top} \\: . \\tag{20b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using equations [20a](#eq20a), [20b](#eq20b), and the least-squares estimator ([equation 17](#eq17)), we define the covariance matrix $\\mathbf{\\Sigma}_{\\mathbf{p}}$ of the estimated parameter vector $\\tilde{\\mathbf{p}}$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq21'></a>\n",
    "$$\n",
    "\\mathbf{\\Sigma}_{\\mathbf{p}} \n",
    "= \\left[ \\left( \\mathbf{A}^{\\top}\\mathbf{A} \\right)^{-1}\\mathbf{A}^{\\top} \\right] \\mathbf{\\Sigma_{d}} \n",
    "\\left[ \\left( \\mathbf{A}^{\\top}\\mathbf{A} \\right)^{-1}\\mathbf{A}^{\\top} \\right]^{\\top} \\: , \\tag{21}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{\\Sigma_{d}}$ is the covariance matrix of the observed data $\\mathbf{d}$. By considering that the observed data are uncorrelated and have the same variance $\\sigma_{\\mathbf{d}}^{2}$, the covariance matrix $\\mathbf{\\Sigma}_{\\mathbf{p}}$ of the estimated parameter vector $\\tilde{\\mathbf{p}}$ assumes the particular form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eq22'></a>\n",
    "$$\n",
    "\\mathbf{\\Sigma}_{\\mathbf{p}} = \\sigma_{\\mathbf{d}}^{2} \\left( \\mathbf{A}^{\\top}\\mathbf{A} \\right)^{-1} \\: . \\tag{22}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the uncertainty of the $j$-th element of the estimated parameter vector $\\tilde{\\mathbf{p}}$ can be defined as the root square of the $j$-th element of the main diagonal of $\\mathbf{\\Sigma}_{\\mathbf{p}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
